{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "042c56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from typing import Any\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.base import OneToOneFeatureMixin\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40261a4",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a7932",
   "metadata": {},
   "source": [
    "## Dataframe Validation\n",
    "\n",
    "When loading datasets, we want to ensure that each frame only consists of two columns, namely a review and sentiment columns, before concatenating all of the data into a singular frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "122b2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataframe(dataframe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Ensure the dataframe has a 'review' and 'sentiment' column, and no other\n",
    "    column. If any of these is violated, throw an exception.\n",
    "\n",
    "    :param dataframe: The dataframe to validate\n",
    "    \"\"\"\n",
    "    if 'review' not in dataframe.columns:\n",
    "        raise Exception(\n",
    "            'Malformed dataframe',\n",
    "            dataframe,\n",
    "            'doesnt contain \"review\" column'\n",
    "        )\n",
    "    elif 'sentiment' not in dataframe.columns:\n",
    "        raise Exception(\n",
    "            'Malformed dataframe',\n",
    "            dataframe,\n",
    "            'doesnt contain \"sentiment\" column'\n",
    "        )\n",
    "    elif len(dataframe.columns) != 2:\n",
    "        raise Exception(\n",
    "            'Malformed dataframe',\n",
    "            dataframe,\n",
    "            'contains more than 2 columns'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a6594",
   "metadata": {},
   "source": [
    "## Dataframe Loading\n",
    "\n",
    "We want to be able to pass a list of tuple, where the first element is the dataframe to process, and the second is an encoder to apply to the sentiment column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "106b80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "        dataframe_encoder_pairs: list[(pd.DataFrame, OneToOneFeatureMixin)],\n",
    "        max_sequence_length: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a dataframe containing the reviews and sentiment scores of every\n",
    "    provided dataframe by applying each encoder to its corresponding dataset\n",
    "    and concatenating them along the horizontal axis.\n",
    "\n",
    "    :param dataframe_encoder_pairs: The datasets and their corresponding\n",
    "    encoders\n",
    "    \"\"\"\n",
    "    full_frame = pd.DataFrame({\n",
    "        'review': np.array([], dtype=str),\n",
    "        'sentiment': np.array([], dtype=np.int32)\n",
    "    })\n",
    "\n",
    "    for dataframe, encoder in dataframe_encoder_pairs:\n",
    "        # incase the dataframe uses object or some other data type\n",
    "        dataframe['review'] = dataframe['review'].astype('str')\n",
    "\n",
    "        # validate dataframe, clean the data and encode the sentiment scores\n",
    "        validate_dataframe(dataframe)\n",
    "        encoded_sentiment = encode_sentiment(\n",
    "            dataframe['sentiment'].to_numpy(),\n",
    "            encoder\n",
    "        )\n",
    "        cleaned_reviews = clean_reviews(dataframe['review'].to_numpy())\n",
    "\n",
    "        # join the data into a single frame, and tack it on the full frame\n",
    "        new_data = np.concatenate(\n",
    "            [cleaned_reviews, encoded_sentiment],\n",
    "            axis=1\n",
    "        )\n",
    "        full_frame = np.concatenate([full_frame, new_data])\n",
    "\n",
    "    full_frame = pd.DataFrame(\n",
    "        full_frame,\n",
    "        columns=['review', 'sentiment']\n",
    "    )\n",
    "    \n",
    "    return full_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625e37c",
   "metadata": {},
   "source": [
    "## String Embedding\n",
    "\n",
    "Use Google's Word2Vec model to create an embedding matrix for the final model. Also pads reviews with too few characters, encodes sentiment values, and returns thesize of the vocabulary associated with the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10dc6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_dataset(full_frame) -> (np.ndarray, np.ndarray, np.ndarray, int):\n",
    "    # 2d list of sentence, where each element is a string, use this to\n",
    "    # train a word to vector model\n",
    "    if not os.path.isfile('./saved_models/w2vmodel.kvmodel'):\n",
    "        documents = [_text.split() for _text in full_frame.review]\n",
    "        w2v_model = gensim.models.word2vec.Word2Vec(vector_size=300,\n",
    "                                                    window=7,\n",
    "                                                    min_count=10,\n",
    "                                                    workers=8)\n",
    "        w2v_model.build_vocab(documents)\n",
    "        w2v_model.train(documents, total_examples=len(documents), epochs=32)\n",
    "        w2v_model.save('./saved_models/w2vmodel.kvmodel')\n",
    "    else:\n",
    "        w2v_model = gensim.models.word2vec.Word2Vec.load('./saved_models/w2vmodel.kvmodel')\n",
    "\n",
    "    # fit a tokenizer to the reviews\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(full_frame.review)\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # pad the reviews\n",
    "    padded_review = pad_sequences(\n",
    "        tokenizer.texts_to_sequences(full_frame.review),\n",
    "        maxlen=300\n",
    "    )\n",
    "\n",
    "    # encode sentiments\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(full_frame.sentiment.tolist())\n",
    "\n",
    "    encoded_sentiment = encoder.transform(full_frame.sentiment.tolist())\n",
    "\n",
    "    # create embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in w2v_model.wv:\n",
    "            embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "    return padded_review, encoded_sentiment, embedding_matrix, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639c29c",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Remove unncessary white space, @, hyperlinks and non-alphanumeric characters from any strings. Also make every thing lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5b775e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean a review\n",
    "\n",
    "    :param review: string to be cleaned\n",
    "    :return: Cleaned review\n",
    "    \"\"\"\n",
    "\n",
    "    # remove punctuation from X\n",
    "    strip_punct = str.maketrans('', '', string.punctuation)\n",
    "    review = review.translate(strip_punct)\n",
    "\n",
    "    # replace double spaces with single spaces and remove spaces at the end of\n",
    "    # sentences\n",
    "    review = re.sub(' +', ' ', review)\n",
    "    review = re.sub(' $', '', review)\n",
    "    review = re.sub('^ ', '', review)\n",
    "\n",
    "    # remove @, links, and non-alphanumeric characters\n",
    "    review = re.sub('@\\\\S+|https?:\\\\S+|http?:\\\\S|[^A-Za-z0-9]+', ' ', review)\n",
    "\n",
    "    # lower case everything\n",
    "    review = review.lower()\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "431664ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews(reviews: np.ndarray) -> np.ndarray:\n",
    "    return np.array(list(map(clean_review, list(reviews)))).reshape((-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b292f",
   "metadata": {},
   "source": [
    "## Sentiment Scaling\n",
    "\n",
    "Scale sentiment scores so that they range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dac3b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentiment(sentiment: np.ndarray,\n",
    "                     encoder: Any) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encode sentiment scores and scale them such that they have a max of 1,\n",
    "    and min of 0\n",
    "\n",
    "    :param sentiment: Sentiment scores\n",
    "    :param encoder: Encoder\n",
    "    :return: Sentiment scores with the encoder applied to them, also scaled\n",
    "    to be within 0 to 1\n",
    "    \"\"\"\n",
    "    if encoder is not None:\n",
    "        encoded_sentiment = encoder.fit_transform(\n",
    "            sentiment.reshape(-1, 1)\n",
    "        ).astype(np.int64)\n",
    "    else:\n",
    "        encoded_sentiment = sentiment.reshape((-1, 1))\n",
    "\n",
    "    encoded_sentiment = MinMaxScaler().fit_transform(encoded_sentiment)\n",
    "    return encoded_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea17a2",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a512c",
   "metadata": {},
   "source": [
    "## Base Model\n",
    "\n",
    "Create a model that combines CNNs and RNNs to perform sentiment analysis, as originally proposed by [Xingyou Wang, Weijie Jiang, Zhiyong Luo](https://aclanthology.org/C16-1229.pdf) in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_matrix: np.ndarray, vocab_size: int):\n",
    "    input_layer = keras.layers.Input(shape=(300,))\n",
    "    my_embed_layer = keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)(input_layer)\n",
    "    dropout_layer_1 = keras.layers.Dropout(0.5)(my_embed_layer)\n",
    "\n",
    "    conv_11 = keras.layers.Conv1D(50, kernel_size=3, padding='same', kernel_initializer='he_uniform')(dropout_layer_1)\n",
    "    max_pool_1 = keras.layers.MaxPool1D(padding='same')(conv_11)\n",
    "\n",
    "    conv_21 = keras.layers.Conv1D(50, kernel_size=3, padding='same', kernel_initializer='he_uniform')(dropout_layer_1)\n",
    "    max_pool_2 = keras.layers.MaxPool1D(padding='same')(conv_21)\n",
    "\n",
    "    concat = keras.layers.concatenate([max_pool_1, max_pool_2], axis=1)\n",
    "    dropout_layer_2 = keras.layers.Dropout(0.15)(concat)\n",
    "\n",
    "    gru = tf.compat.v1.keras.layers.GRU(128)(dropout_layer_2)\n",
    "    dense = keras.layers.Dense(400)(gru)\n",
    "    dropout_layer_3 = keras.layers.Dropout(0.1)(dense)\n",
    "    out = keras.layers.Dense(1, activation='sigmoid')(dropout_layer_3)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea0ea2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de631b43",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa3b328a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imdb_data = pd.read_csv(\n",
    "    './data/imdb_utf8.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1807f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = load_data([\n",
    "    (imdb_data, OrdinalEncoder(categories=[['negative', 'positive']]))\n",
    "], 300)\n",
    "\n",
    "x, y, embedding_matrix, vocab_size = get_encoded_dataset(full_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef286a4",
   "metadata": {},
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d6c43a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36/36 [==============================] - 27s 699ms/step - loss: 0.6219 - accuracy: 0.6435 - val_loss: 0.4606 - val_accuracy: 0.7875\n",
      "Epoch 2/5\n",
      "36/36 [==============================] - 25s 694ms/step - loss: 0.3794 - accuracy: 0.8297 - val_loss: 0.3224 - val_accuracy: 0.8790\n",
      "Epoch 3/5\n",
      "36/36 [==============================] - 25s 693ms/step - loss: 0.3120 - accuracy: 0.8674 - val_loss: 0.3040 - val_accuracy: 0.8925\n",
      "Epoch 4/5\n",
      "36/36 [==============================] - 25s 690ms/step - loss: 0.3011 - accuracy: 0.8723 - val_loss: 0.3015 - val_accuracy: 0.8940\n",
      "Epoch 5/5\n",
      "36/36 [==============================] - 25s 695ms/step - loss: 0.2911 - accuracy: 0.8774 - val_loss: 0.2843 - val_accuracy: 0.8968\n"
     ]
    }
   ],
   "source": [
    "model = create_model(embedding_matrix, vocab_size)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=1000, validation_split=0.1)\n",
    "model.save('./saved_models/sentiment.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7b4d6",
   "metadata": {},
   "source": [
    "## Generate Predictions And Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96fa0d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 15s 144ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a46ea567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8917\n",
      "Precision : 0.9150905432595573\n",
      "Recall : 0.8731042426569399\n"
     ]
    }
   ],
   "source": [
    "discrete_preds = np.zeros((len(list(predictions)), 1))\n",
    "discrete_preds[predictions <= 1/2] = 0\n",
    "discrete_preds[predictions > 1/2] = 1\n",
    "print('Accuracy :', accuracy_score(discrete_preds, y_test))\n",
    "print('Precision :', precision_score(discrete_preds, y_test))\n",
    "print('Recall :', recall_score(discrete_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd436d40",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef3ac8",
   "metadata": {},
   "source": [
    "## Create Model With Variables In Place Of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b9e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_model(hp):\n",
    "    hp_conv_units = hp.Int('conv_units', min_value=50, max_value=200, step=75)\n",
    "    hp_kernel_size = hp.Int('kernel_size', min_value=2, max_value=4, step=1)\n",
    "    hp_dense_units = hp.Int('dense_units', min_value=100, max_value=200, step=100)\n",
    "    \n",
    "    input_layer = keras.layers.Input(shape=(300,))\n",
    "    my_embed_layer = keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)(input_layer)\n",
    "    dropout_layer_1 = keras.layers.Dropout(0.5)(my_embed_layer)\n",
    "\n",
    "    conv_11 = keras.layers.Conv1D(hp_conv_units, kernel_size=hp_kernel_size, padding='same', kernel_initializer='he_uniform')(dropout_layer_1)\n",
    "    max_pool_1 = keras.layers.MaxPool1D(padding='same')(conv_11)\n",
    "\n",
    "    conv_21 = keras.layers.Conv1D(hp_conv_units, kernel_size=hp_kernel_size, padding='same', kernel_initializer='he_uniform')(dropout_layer_1)\n",
    "    max_pool_2 = keras.layers.MaxPool1D(padding='same')(conv_21)\n",
    "\n",
    "    concat = keras.layers.concatenate([max_pool_1, max_pool_2], axis=1)\n",
    "    dropout_layer_2 = keras.layers.Dropout(0.15)(concat)\n",
    "\n",
    "    gru = keras.layers.Bidirectional(tf.compat.v1.keras.layers.GRU(128))(dropout_layer_2)\n",
    "    dense = keras.layers.Dense(hp_dense_units)(gru)\n",
    "    dropout_layer_3 = keras.layers.Dropout(0.1)(dense)\n",
    "    out = keras.layers.Dense(1, activation='sigmoid')(dropout_layer_3)\n",
    "\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "                   metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182b0be",
   "metadata": {},
   "source": [
    "## Search For Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31f53bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 08m 30s]\n",
      "val_loss: 0.36807867884635925\n",
      "\n",
      "Best val_loss So Far: 0.32960665225982666\n",
      "Total elapsed time: 08h 39m 20s\n"
     ]
    }
   ],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.05, patience=3)\n",
    "tuner = kt.Hyperband(create_bidirectional_model,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=5)\n",
    "tuner.search(x_train[:5000], y_train[:5000], epochs=3, batch_size=75, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73944949",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed555227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv_units': 125,\n",
       " 'kernel_size': 3,\n",
       " 'dense_units': 200,\n",
       " 'tuner/epochs': 5,\n",
       " 'tuner/initial_epoch': 2,\n",
       " 'tuner/bracket': 1,\n",
       " 'tuner/round': 1,\n",
       " 'tuner/trial_id': '0003'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hp.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b40238",
   "metadata": {},
   "source": [
    "## Create The Final Model Based on New Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19ccff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_model():\n",
    "    input_layer = keras.layers.Input(shape=(300,))\n",
    "    my_embed_layer = keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)(input_layer)\n",
    "    dropout_layer_1 = keras.layers.Dropout(0.5)(my_embed_layer)\n",
    "\n",
    "    conv_11 = keras.layers.Conv1D(125, kernel_size=3, padding='same', kernel_initializer='he_uniform')(dropout_layer_1)\n",
    "    max_pool_1 = keras.layers.MaxPool1D(padding='same')(conv_11)\n",
    "\n",
    "    conv_21 = keras.layers.Conv1D(125, kernel_size=3, padding='same', kernel_initializer='he_uniform')(dropout_layer_1)\n",
    "    max_pool_2 = keras.layers.MaxPool1D(padding='same')(conv_21)\n",
    "\n",
    "    concat = keras.layers.concatenate([max_pool_1, max_pool_2], axis=1)\n",
    "    dropout_layer_2 = keras.layers.Dropout(0.15)(concat)\n",
    "\n",
    "    gru = tf.compat.v1.keras.layers.GRU(128)(dropout_layer_2)\n",
    "    dense = keras.layers.Dense(200)(gru)\n",
    "    dropout_layer_3 = keras.layers.Dropout(0.1)(dense)\n",
    "    out = keras.layers.Dense(1, activation='sigmoid')(dropout_layer_3)\n",
    "\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "                   metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b3ccd",
   "metadata": {},
   "source": [
    "## Train Final Model On Full Train Set And Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8774f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = create_final_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da54e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "36/36 [==============================] - 59s 2s/step - loss: 0.2756 - accuracy: 0.8838 - val_loss: 0.2875 - val_accuracy: 0.8940\n",
      "Epoch 2/4\n",
      "36/36 [==============================] - 58s 2s/step - loss: 0.2652 - accuracy: 0.8879 - val_loss: 0.2794 - val_accuracy: 0.8978\n",
      "Epoch 3/4\n",
      "36/36 [==============================] - 58s 2s/step - loss: 0.2623 - accuracy: 0.8913 - val_loss: 0.2695 - val_accuracy: 0.9030\n",
      "Epoch 4/4\n",
      "36/36 [==============================] - 58s 2s/step - loss: 0.2573 - accuracy: 0.8934 - val_loss: 0.2560 - val_accuracy: 0.9065\n"
     ]
    }
   ],
   "source": [
    "final_model.fit(x_train, y_train, epochs=4, batch_size=1000, validation_split=0.1)\n",
    "final_model.save('./final_models/final.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f9f15cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 29s 147ms/step\n",
      "Accuracy : 0.9015\n",
      "Precision : 0.9114688128772636\n",
      "Recall : 0.8926108374384236\n"
     ]
    }
   ],
   "source": [
    "predictions_final = final_model.predict(x_test, batch_size=50)\n",
    "discrete_preds_final = np.zeros((len(list(predictions_final)), 1))\n",
    "discrete_preds_final[predictions_final <= 1/2] = 0\n",
    "discrete_preds_final[predictions_final > 1/2] = 1\n",
    "print('Accuracy :', accuracy_score(discrete_preds_final, y_test))\n",
    "print('Precision :', precision_score(discrete_preds_final, y_test))\n",
    "print('Recall :', recall_score(discrete_preds_final, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfdml_plugin",
   "language": "python",
   "name": "tfdml_plugin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
